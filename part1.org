#+OPTIONS: H:7
#+LaTeX_CLASS: koma-article
#+TITLE: My little document

* root
** Reinforcement Learning
***  Q Learning
$$
Q(s_{t},a_{t})\
\leftarrow\
 \underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}\
+\underbrace {\alpha } _{\rm {learning~rate}}\
\cdot \left(\
\overbrace {\underbrace {r_{t+1}} _{\rm {reward}}+\underbrace {\gamma } _{\rm {discount~factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\rm {estimate~of~optimal~future~value}}} ^{\rm {learned~value}}-\underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}\right)$$
** The video
   https://www.youtube.com/watch?v=zOgSC---rgM
   
   #+CAPTION: youtube_screenshot
   #+ATTR_LATEX: :width 10cm
   #+NAME:   fig:youtube_screenshot
   [[file:images/youtube_screenshot.png]]
   
   The video illustrates a car learning to avoid obstacles. 
   As shown in Figure [[fig:youtube_screenshot]], the environment is a 2D scenario.
   The whole scenario is surrounded by fences.
   There are 4 irregularly shaped obstacles.
   To show the learned ability generalizes well with different layouts of obstacles,
   the obstacles will revolve about the center of the room slowly,
   at a constant speed, during running.

   For each period, the car starts from the center of the room,
   with a randomly chosen direction.
   Its action is controlled by a reinforcement learning algorithm.
   During the early periods, the actions are like randomly decided.
   When the car runs into obstacles or fences,
   it will be repositioned to the center of the room, and a new period begins.
   The car learns over time.
   
   #+CAPTION: youtube_structure
   #+ATTR_LATEX: :width 10cm
   #+NAME:   fig:youtube_structure
   [[file:images/youtube_structure.png]]
 
   The illustration comprises two parts, a 2D environment described above,
   and a learning algorithm which controls the action of the car.
   The algorithm learns from and makes decisions on specified data provided by the environment.
   The environment emulates 5 state sensors,
   corresponding to 5 different direction in front of the car.
   These sensors find the nearest obstacle,
   and measure the distance from the obstacle to the car.
   The distance information is transferred to the car in the form of a 5D vector.
   During running, the algorithm will predict an action based on the distance information 
   and then feed it back to the environment, to control the movement of the car. 
 
** Re-Implementation
   For our further research, we need a verified code base to start with. 
   So we want to implement the application in this video. 
    
   #+CAPTION: structure
   #+ATTR_LATEX: :width 10cm
   #+NAME:   fig:reimplementation_structure
   [[file:images/reimplementation_structure.png]]
 
   The implementation takes 3 steps.
   - Build an environment.
   - Implement a reinforcement learning algorithm to control the car.
   - Provide and transfer learning data and actions between the environment and the algorithm.

*** Reinforcement Learning 
    We started this part with a project on Github.com.
    In further research, we will build a variant algorithm of the deep q network.
    So we choose this implementation of deep q network as the code base of our reinforcement learning algorithm.
    This implementation is written in python, and based on Theano and Lasagne.
    - Paper :: http://arxiv.org/abs/1312.5602
    - Code :: https://github.com/spragunr/deep_q_rl
**** Adaptation
     The deep q network takes the raw images of a game as its inputs to predict actions.
     The network includes convolutional layers to handle the image data.
     But the current environment provides only distance information, 
     in the form of a vector, 
     instead of the raw images.
     So we replaced the convolutional neural network with a simple multi-layer neural network,
     to handle the distance information.
*** The Environment
    We choose to build this 2D environment on a real-time 3D rendering engine. 
    It is for the convenience of later transition.
    Because our later goal is to train the car to avoid obstacles in 3D environments. 
**** Panda3D
     The 3D rendering task will be accomplished by Panda3D, a game development environment.
     It is written in C++ and supports both C++ and python.
***** Intra Process Communication
      In future research, 
      it is required to frequently transfer raw images from the environment to the learning model.
      To make this process more efficient, we want to be able to transfer image data as an intra-process pointer.
      Choosing python as the programming language for both the environment and the learning model 
      will allows us to conveniently implement this mechanism.
**** Models
     The 3D models used to build the 3D environment come from Panda3D's example games.
     - The models of the room are provided by the example bump-mapping. 
     - The models of the chess pieces are provided by the example chessboard.
**** Layout 
     
     #+CAPTION: A 3D first person view of the car
     #+ATTR_LATEX: :width 10cm
     #+NAME:   fig:screenshot_first_person
     [[file:images/screenshot_first_person.png]]

     #+CAPTION: A top-down view. Red shapes represents obstacles. (mark the poles later)(Green circle/blue lines remove later) Green destination(remove)Blue line routes (remove)
     #+ATTR_LATEX: :width 10cm
     #+NAME:   fig:screenshot_layout_2D
     [[file:images/screenshot_layout_2D.png]]

     As shown in Figure [[fig:screenshot_first_person]], the scenario is located in a cubic room.
     A round pole is fixed at the left bottom corner (Figure [[fig:screenshot_layout_2D]]).
     And a square pole is fixed at the top right corner.
     200 chess pieces are randomly positioned in the room.

*** Learning Data
 
    #+CAPTION: cropping the horizontal line from the depth map
    #+ATTR_LATEX: :width 5cm
    #+NAME:   fig:screenshot_first_person
    [[file:images/depth_map_1d.png]]

**** Depth Map

     #+CAPTION: A depth map. Light colors represents for rear objects. Dark colors represents for near objects.
     #+ATTR_LATEX: :width 5cm
     #+NAME:   fig:depth_map
     [[file:images/depth_map.png]]

     A depth map is a type of distance information.
     Like a raw image, it can be represented by a real-valued matrix. 
     Corresponding to a raw image (Figure [[fig:screenshot_first_person]]), 
     each point on a depth map (Figure [[fig:depth_map]]) represents the distance from that point to the viewpoint.

     In real world, there are many algorithms able to generate depth maps from raw images.
     Within a 3D rendering engine, a depth map can be generated by calculating the distance to a nearest 3D model for all directions.
     Depth maps are widely used in 3D rendering algorithms. For example, they are necessary intermediate data in shadow algorithms.
     Rendering depth maps with 3D models is a common seen function of a 3D engine, like Panda3D.

**** 
 How to generate from 3d models
 How to extract distance from depth map. Figure ???

 然后,截取地平线的部分(Figure 3),作为对视频中的一维距离信息的模拟,发送给agent
 在场景中,当agent遇到障碍物的时候,场景会发送惩罚信号给agent.
 Figure 1: agent视角的虚拟三维环境,通过三维引擎的立体视觉模式加入了左右两个视角
 Figure 2: depth map generated from 3d models
 Figure 3: 截取地平线的部分生成一维的深度信息
 o   Reimplement q-learning
 o   Feeding distance to q-learning
 Different Implementaion
 o   Data: volume??
*** Training
**** Random Actions
*** Results and discussion:
    在满是障碍物的地图上,agent很明显的可以在较长的一段时间中避开障碍物. 但是并不能做到在所以情形下完全避开障碍物.
**** Problems
     我们缺乏有效的指标判断训练结果的有效性. 障碍物地图是随机生成的,有些情况下agent可能陷入无法避开障碍物的情形.但是我们缺乏手段评估哪些障碍物环境是属于这一类别的.
     在参数调整的后期,很难根据agent不碰撞的时间长度来判断算法的优劣
   
   
   
     
