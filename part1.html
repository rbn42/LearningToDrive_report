<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2016-08-31 Wed 08:25 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>My little document</title>
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">My little document</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline27">1. root</a>
<ul>
<li><a href="#orgheadline4">1.1. <span class="todo TODO">TODO</span> Reinforcement Learning</a>
<ul>
<li><a href="#orgheadline1">1.1.1. Action Value Function</a></li>
<li><a href="#orgheadline2">1.1.2. &epsilon;-greedy</a></li>
<li><a href="#orgheadline3">1.1.3. Q Learning</a></li>
</ul>
</li>
<li><a href="#orgheadline5">1.2. The video</a></li>
<li><a href="#orgheadline26">1.3. Re-Implementation</a>
<ul>
<li><a href="#orgheadline7">1.3.1. Reinforcement Learning</a>
<ul>
<li><a href="#orgheadline6">1.3.1.1. Adaptation</a></li>
</ul>
</li>
<li><a href="#orgheadline13">1.3.2. The Environment</a>
<ul>
<li><a href="#orgheadline9">1.3.2.1. Panda3D</a>
<ul>
<li><a href="#orgheadline8">1.3.2.1.1. Intra Process Communication</a></li>
</ul>
</li>
<li><a href="#orgheadline10">1.3.2.2. Layout</a></li>
<li><a href="#orgheadline11">1.3.2.3. Agent Speed</a></li>
<li><a href="#orgheadline12">1.3.2.4. Models</a></li>
</ul>
</li>
<li><a href="#orgheadline18">1.3.3. Learning Data</a>
<ul>
<li><a href="#orgheadline14">1.3.3.1. Depth Maps</a></li>
<li><a href="#orgheadline17">1.3.3.2. Collision Detection</a>
<ul>
<li><a href="#orgheadline15">1.3.3.2.1. Built-In Engine of Panda3D</a></li>
<li><a href="#orgheadline16">1.3.3.2.2. Bullet</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline23">1.3.4. Training Strategy</a>
<ul>
<li><a href="#orgheadline19">1.3.4.1. Rewarding Rules</a></li>
<li><a href="#orgheadline20">1.3.4.2. Allowed Actions</a></li>
<li><a href="#orgheadline21">1.3.4.3. Random Actions</a></li>
<li><a href="#orgheadline22">1.3.4.4. History Pool</a></li>
</ul>
</li>
<li><a href="#orgheadline25">1.3.5. Results and discussion</a>
<ul>
<li><a href="#orgheadline24">1.3.5.1. Problems</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline27" class="outline-2">
<h2 id="orgheadline27"><span class="section-number-2">1</span> root</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgheadline4" class="outline-3">
<h3 id="orgheadline4"><span class="section-number-3">1.1</span> <span class="todo TODO">TODO</span> Reinforcement Learning</h3>
<div class="outline-text-3" id="text-1-1">
</div><div id="outline-container-orgheadline1" class="outline-4">
<h4 id="orgheadline1"><span class="section-number-4">1.1.1</span> Action Value Function</h4>
</div>
<div id="outline-container-orgheadline2" class="outline-4">
<h4 id="orgheadline2"><span class="section-number-4">1.1.2</span> &epsilon;-greedy</h4>
</div>
<div id="outline-container-orgheadline3" class="outline-4">
<h4 id="orgheadline3"><span class="section-number-4">1.1.3</span> Q Learning</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
\[
Q(s_{t},a_{t})\
\leftarrow\
 \underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}\
+\underbrace {\alpha } _{\rm {learning~rate}}\
\cdot \left(\
\overbrace {\underbrace {r_{t+1}} _{\rm {reward}}+\underbrace {\gamma } _{\rm {discount~factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\rm {estimate~of~optimal~future~value}}} ^{\rm {learned~value}}-\underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}\right)\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5"><span class="section-number-3">1.2</span> The video</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://www.youtube.com/watch?v=zOgSC---rgM">https://www.youtube.com/watch?v=zOgSC---rgM</a>
</p>


<div id="orgparagraph1" class="figure">
<p><img src="images/youtube_screenshot.png" alt="youtube_screenshot.png" />
</p>
<p><span class="figure-number">Figure 1:</span> youtube<sub>screenshot</sub></p>
</div>

<p>
The video illustrates a car learning to avoid obstacles. 
As shown in Figure <a href="#orgparagraph1">1</a>, the environment is a 2D scenario.
The whole scenario is surrounded by fences.
There are 4 irregularly shaped obstacles.
To show the learned ability generalizes well with different layouts of obstacles,
the obstacles will circle around the center of the room slowly,
at a constant speed, during running.
</p>

<p>
For each period, the car starts from the center of the room,
with a randomly chosen direction.
Its action is controlled by a reinforcement learning algorithm.
During the early periods, the actions are like randomly decided.
When the car runs into obstacles or fences,
it will be repositioned to the center of the room, and a new period begins.
The car learns over time.
</p>


<div id="orgparagraph2" class="figure">
<p><img src="images/youtube_structure.png" alt="youtube_structure.png" />
</p>
<p><span class="figure-number">Figure 2:</span> youtube<sub>structure</sub></p>
</div>

<p>
The illustration comprises two parts, a 2D environment described above,
and a learning algorithm which controls the action of the car.
The algorithm learns from and makes decisions on specified data provided by the environment.
The environment emulates 5 state sensors,
corresponding to 5 different direction in front of the car.
These sensors find the nearest obstacle,
and measure the distance from the obstacle to the car.
The distance information is transferred to the car in the form of a 5D vector.
During running, the algorithm will predict an action based on the distance information 
and then feed it back to the environment, to control the movement of the car. 
</p>
</div>
</div>

<div id="outline-container-orgheadline26" class="outline-3">
<h3 id="orgheadline26"><span class="section-number-3">1.3</span> Re-Implementation</h3>
<div class="outline-text-3" id="text-1-3">
<p>
For our further research, we need a verified code base to start with. 
So we want to implement the application in this video. 
</p>


<div id="orgparagraph3" class="figure">
<p><img src="images/reimplementation_structure.png" alt="reimplementation_structure.png" />
</p>
<p><span class="figure-number">Figure 3:</span> structure</p>
</div>

<p>
The implementation takes 3 steps.
</p>
<ul class="org-ul">
<li>Build an environment.</li>
<li>Implement a reinforcement learning algorithm to control the car.</li>
<li>Provide and transfer learning data and actions between the environment and the algorithm.</li>
</ul>
</div>

<div id="outline-container-orgheadline7" class="outline-4">
<h4 id="orgheadline7"><span class="section-number-4">1.3.1</span> Reinforcement Learning</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
We started this part with a project on Github.com.
In further research, we will build a variant algorithm of the deep q network.
So we choose this implementation of deep q network as the code base of our reinforcement learning algorithm.
This implementation is written in Python, and based on Theano and Lasagne.
</p>
<dl class="org-dl">
<dt>Paper</dt><dd><a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a></dd>
<dt>Code</dt><dd><a href="https://github.com/spragunr/deep_q_rl">https://github.com/spragunr/deep_q_rl</a></dd>
</dl>
</div>
<div id="outline-container-orgheadline6" class="outline-5">
<h5 id="orgheadline6"><span class="section-number-5">1.3.1.1</span> Adaptation</h5>
<div class="outline-text-5" id="text-1-3-1-1">
<p>
The deep q network takes the raw images of a game as its inputs to predict actions.
The network includes convolutional layers to handle the image data.
But the current environment provides only distance information, 
in the form of a vector, 
instead of the raw images.
So we replaced the convolutional neural network with a simple multi-layer neural network,
to handle the distance information.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="orgheadline13"><span class="section-number-4">1.3.2</span> The Environment</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
We choose to build this 2D environment on a real-time 3D rendering engine. 
It is for the convenience of later transition.
Because our later goal is to train the car to avoid obstacles in 3D environments. 
</p>
</div>
<div id="outline-container-orgheadline9" class="outline-5">
<h5 id="orgheadline9"><span class="section-number-5">1.3.2.1</span> Panda3D</h5>
<div class="outline-text-5" id="text-1-3-2-1">
<p>
The 3D rendering task will be accomplished by Panda3D, a game development environment.
It is written in C++ and supports developments in both C++ and Python.
</p>
</div>
<div id="outline-container-orgheadline8" class="outline-6">
<h6 id="orgheadline8"><span class="section-number-6">1.3.2.1.1</span> Intra Process Communication</h6>
<div class="outline-text-6" id="text-1-3-2-1-1">
<p>
In future research, 
it is required to frequently transfer raw images from the environment to the learning model.
To make this process more efficient, we want to be able to transfer image data in the form of intra-process pointers.
Choosing Python as the programming language for both the virtual environment and the learning model 
will allows us to conveniently implement this mechanism.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline10" class="outline-5">
<h5 id="orgheadline10"><span class="section-number-5">1.3.2.2</span> Layout</h5>
<div class="outline-text-5" id="text-1-3-2-2">

<div id="orgparagraph4" class="figure">
<p><img src="images/screenshot_first_person.png" alt="screenshot_first_person.png" />
</p>
<p><span class="figure-number">Figure 4:</span> A 3D first person view of the car</p>
</div>


<div id="orgparagraph5" class="figure">
<p><img src="images/screenshot_layout_2D.png" alt="screenshot_layout_2D.png" />
</p>
<p><span class="figure-number">Figure 5:</span> A top-down view of a layout. Red shapes represents for obstacles. (mark the poles later)(Green circle/blue lines remove later) Green destination(remove)Blue line routes (remove)</p>
</div>

<p>
As shown in Figure <a href="#orgparagraph4">4</a>, the scenario is located in a cubic room.
The size of the room is 120x120.
A round pole is fixed at the left bottom corner (Figure <a href="#orgparagraph5">5</a>).
Its radius is 12.5.
And a square pole is fixed at the top right corner. 
Its area size is 40x40.
200 chess pieces are randomly positioned in the room.
Their radius distribute randomly between 0.5 and 5.
</p>
</div>
</div>
<div id="outline-container-orgheadline11" class="outline-5">
<h5 id="orgheadline11"><span class="section-number-5">1.3.2.3</span> Agent Speed</h5>
<div class="outline-text-5" id="text-1-3-2-3">
<p>
The fps of the 3D environment is 60. 
The agent is allowed to choose an action for every 4 frames.
The speed of the agent is 6. 
So, for each action step, the distance the agent can travel is 0.4.
</p>
</div>
</div>
<div id="outline-container-orgheadline12" class="outline-5">
<h5 id="orgheadline12"><span class="section-number-5">1.3.2.4</span> Models</h5>
<div class="outline-text-5" id="text-1-3-2-4">
<p>
The 3D models used to build the 3D environment come from Panda3D's example games.
</p>
<ul class="org-ul">
<li>The models of the room are provided by the example bump-mapping.</li>
<li>The models of the chess pieces are provided by the example chessboard.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgheadline18" class="outline-4">
<h4 id="orgheadline18"><span class="section-number-4">1.3.3</span> Learning Data</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
Required learning data includes distance information and collision signals.
</p>
</div>
<div id="outline-container-orgheadline14" class="outline-5">
<h5 id="orgheadline14"><span class="section-number-5">1.3.3.1</span> Depth Maps</h5>
<div class="outline-text-5" id="text-1-3-3-1">
<p>
Within a 3D render engine, a convenient way to generate distance information 
is to extract a depth map from the 3D models.
</p>


<div id="orgparagraph6" class="figure">
<p><img src="images/depth_map.png" alt="depth_map.png" />
</p>
<p><span class="figure-number">Figure 6:</span> A depth map. Light colors represents for rear objects. Dark colors represents for near objects.</p>
</div>

<p>
A depth map is a type of distance information.
Like a raw image, it can be represented by a real-valued matrix. 
Corresponding to a raw image (Figure <a href="#orgparagraph4">4</a>), 
each point on a depth map (Figure <a href="#orgparagraph6">6</a>) represents the distance from that point to the viewpoint.
</p>

<p>
In real world, there are many algorithms able to generate depth maps from raw images.
Within a 3D rendering engine, a depth map can be generated by calculating the distance to a nearest 3D model for all directions.
Depth maps are widely used in 3D rendering algorithms. 
For example, they are necessary intermediate data in shadow mapping algorithms.
Rendering depth maps with 3D models is a basic function of a 3D engine, like Panda3D.
</p>


<div id="orgparagraph7" class="figure">
<p><img src="images/depth_map_1d.png" alt="depth_map_1d.png" />
</p>
<p><span class="figure-number">Figure 7:</span> Crop the horizon (the red rectangle area) from the depth map</p>
</div>

<p>
By cropping the horizon (Figure <a href="#orgparagraph7">7</a>), the depth map can be converted into 1D distance information.
In our experiments, the pixel size of the depth map is 90x90 
and the size of the distance information vector is 90, 
in contrast to the only 5 state sensors existing in the original video.  
</p>
</div>
</div>
<div id="outline-container-orgheadline17" class="outline-5">
<h5 id="orgheadline17"><span class="section-number-5">1.3.3.2</span> Collision Detection</h5>
<div class="outline-text-5" id="text-1-3-3-2">
<p>
Collision detection is normally a part of a physics engine and 
there are several physics engines integrated within Panda3D.  
</p>
</div>
<div id="outline-container-orgheadline15" class="outline-6">
<h6 id="orgheadline15"><span class="section-number-6">1.3.3.2.1</span> Built-In Engine of Panda3D</h6>
<div class="outline-text-6" id="text-1-3-3-2-1">
<p>
There are two ways to go about collision detection. One is to manually create simple
collision geometries, like spheres and polygons,  for the obstacles.
Panda3D offers built-in collision detection that calculates the impacts 
between these geometries. 
It is fast, but unable to precisely depict collisions of 
complex models. When the agent is only allowed to move around on
the 2D plane of our virtual room, this method works fine. 
Because all the models can be precisely depicted by circles and lines 
in a top-down view. 
</p>
</div>
</div>
<div id="outline-container-orgheadline16" class="outline-6">
<h6 id="orgheadline16"><span class="section-number-6">1.3.3.2.2</span> Bullet</h6>
<div class="outline-text-6" id="text-1-3-3-2-2">
<p>
Another way is to create collision geometries for any models used for 
graphic rendering. Panda3D offers interface for the physics engine Bullet,
and Bullet can generate these collision geometries. 
But it was found that Bullet cannot precisely detect collisions with these 
auto-generated geometries. 
Bullet might send signals of collisions 
when collisions actually didn't happen. 
We have applied the same machine learning algorithms on both engines in a scenario, which allows the 
agent to move on a 2D plane only. 
Comparing to the built-in engine of Panda3D,
Bullet's collision detection lowered down the quality of the training samples seriously, 
and eventually prevented our attempts to train machine learning models with Bullet.
And with the built-in engine, we have to restrict the agent's movement on the 2D plane to 
keep the complexity of collision detection at a low level, which can be handled as simple 
geometries.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgheadline23" class="outline-4">
<h4 id="orgheadline23"><span class="section-number-4">1.3.4</span> Training Strategy</h4>
<div class="outline-text-4" id="text-1-3-4">

<div id="orgparagraph8" class="figure">
<p><img src="images/Artificial_neural_network.png" alt="Artificial_neural_network.png" width="100%" />
</p>
<p><span class="figure-number">Figure 8:</span> Multi-Layer Neural Network(wikipedia.org)</p>
</div>


<div id="orgparagraph9" class="figure">
<p><img src="images/nn_2d.digraph.png" alt="nn_2d.digraph.png" width="100%" />
</p>
<p><span class="figure-number">Figure 9:</span> Multi-Layer Neural Network</p>
</div>

<div class="org-src-container">

<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #ffff00;">input_layer</span> = DataLayeaar()
<span class="linenr"> 2: </span><span style="color: #ff0000;"># </span><span style="color: #ff0000;">normalize input data</span>
<span class="linenr"> 3: </span><span style="color: #ffff00;">input_layer</span> = Layer(input_layer, <span style="color: #0000ff; font-weight: bold;">filter</span>=<span style="color: #00ffff; font-weight: bold;">lambda</span> x: (
<span class="linenr"> 4: </span>    x - T.mean(x)) / T.std(x))
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #ffff00;">layer1</span> = DenseLayer(input_layer=input_layer, n_in=84,
<span class="linenr"> 7: </span>                    n_out=32, std=.005, bias=0)
<span class="linenr"> 8: </span><span style="color: #ffff00;">layer1</span> = ReLU(layer1)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #ffff00;">layer2</span> = DenseLayer(input_layer=layer1, n_in=32, n_out=32, std=.005, bias=0)
<span class="linenr">11: </span><span style="color: #ffff00;">layer2</span> = ReLU(layer2)
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #ffff00;">output_layer</span> = DenseLayer(input_layer=layer2, n_in=32,
<span class="linenr">14: </span>                          n_out=n_actions, std=.005, bias=0)
</pre>
</div>
</div>

<div id="outline-container-orgheadline19" class="outline-5">
<h5 id="orgheadline19"><span class="section-number-5">1.3.4.1</span> Rewarding Rules</h5>
<div class="outline-text-5" id="text-1-3-4-1">
<p>
For non-collision frames, the rewards are 0.0. 
For collision frames, the rewards are -1.0.
This rule set is designed only to inform the agent to avoid obstacles.
These rules will restrict the expected convergent results 
of the action value function between -1.0 and 0.0.
</p>
</div>
</div>
<div id="outline-container-orgheadline20" class="outline-5">
<h5 id="orgheadline20"><span class="section-number-5">1.3.4.2</span> Allowed Actions</h5>
<div class="outline-text-5" id="text-1-3-4-2">
<p>
Allowed Actions include turning left, turning right and going forward, same as the design of the video.
Staying or going backward are not allowed. Because these actions will provide an option for the agent to stay in safe area, without showing its ability of avoiding obstacles.
</p>
</div>
</div>
<div id="outline-container-orgheadline21" class="outline-5">
<h5 id="orgheadline21"><span class="section-number-5">1.3.4.3</span> Random Actions</h5>
<div class="outline-text-5" id="text-1-3-4-3">
<p>
According to the &epsilon;-greedy exploration method.
Actions are randomly decided at the beginning. 
Random actions are later gradually replaced with the best results predicted by the action value function.
</p>
</div>
</div>
<div id="outline-container-orgheadline22" class="outline-5">
<h5 id="orgheadline22"><span class="section-number-5">1.3.4.4</span> History Pool</h5>
<div class="outline-text-5" id="text-1-3-4-4">
<p>
This is a design preserved from the deep Q-network algorithm.
The learning data generated by the environment will be collected into a history pool.
The pool has a fixed size. When its maximum size is reached, old data will be removed.
The training phase will randomly pick data from the pool.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline25" class="outline-4">
<h4 id="orgheadline25"><span class="section-number-4">1.3.5</span> Results and discussion</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
Most of the time, the agent is able to survive and avoid obstacles for over 10,000 steps.
</p>
</div>
<div id="outline-container-orgheadline24" class="outline-5">
<h5 id="orgheadline24"><span class="section-number-5">1.3.5.1</span> Problems</h5>
<div class="outline-text-5" id="text-1-3-5-1">
<p>
We lacks efficient results verification methods. 
The only indicator is the length of the agent's survival time.
It varies largely with different layouts of obstacles, 
and takes a long time to verify.
Because the fps of the environment is fixed at 60.
A single run of simulation over 10,000 steps will consume more than 10 minutes.
</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2016-08-31 Wed 08:25</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
