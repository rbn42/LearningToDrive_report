<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2016-08-20 Sat 06:04 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>My little document</title>
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">My little document</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline18">1. root</a>
<ul>
<li><a href="#orgheadline2">1.1. Reinforcement Learning</a>
<ul>
<li><a href="#orgheadline1">1.1.1. Q Learning</a></li>
</ul>
</li>
<li><a href="#orgheadline3">1.2. The video</a></li>
<li><a href="#orgheadline17">1.3. Re-Implementation</a>
<ul>
<li><a href="#orgheadline5">1.3.1. Reinforcement Learning</a>
<ul>
<li><a href="#orgheadline4">1.3.1.1. Adaptation</a></li>
</ul>
</li>
<li><a href="#orgheadline10">1.3.2. The Environment</a>
<ul>
<li><a href="#orgheadline7">1.3.2.1. Panda3D</a>
<ul>
<li><a href="#orgheadline6">1.3.2.1.1. Intra Process Communication</a></li>
</ul>
</li>
<li><a href="#orgheadline8">1.3.2.2. Models</a></li>
<li><a href="#orgheadline9">1.3.2.3. Layout</a></li>
</ul>
</li>
<li><a href="#orgheadline12">1.3.3. Learning Data</a>
<ul>
<li><a href="#orgheadline11">1.3.3.1. Depth Map</a></li>
</ul>
</li>
<li><a href="#orgheadline14">1.3.4. Training</a>
<ul>
<li><a href="#orgheadline13">1.3.4.1. Random Actions</a></li>
</ul>
</li>
<li><a href="#orgheadline16">1.3.5. Results and discussion:</a>
<ul>
<li><a href="#orgheadline15">1.3.5.1. Problems</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline18" class="outline-2">
<h2 id="orgheadline18"><span class="section-number-2">1</span> root</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgheadline2" class="outline-3">
<h3 id="orgheadline2"><span class="section-number-3">1.1</span> Reinforcement Learning</h3>
<div class="outline-text-3" id="text-1-1">
</div><div id="outline-container-orgheadline1" class="outline-4">
<h4 id="orgheadline1"><span class="section-number-4">1.1.1</span> Q Learning</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
\[
Q(s_{t},a_{t})\
\leftarrow\
 \underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}\
+\underbrace {\alpha } _{\rm {learning~rate}}\
\cdot \left(\
\overbrace {\underbrace {r_{t+1}} _{\rm {reward}}+\underbrace {\gamma } _{\rm {discount~factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\rm {estimate~of~optimal~future~value}}} ^{\rm {learned~value}}-\underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}\right)\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">1.2</span> The video</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://www.youtube.com/watch?v=zOgSC---rgM">https://www.youtube.com/watch?v=zOgSC---rgM</a>
</p>


<div id="orgparagraph1" class="figure">
<p><img src="images/youtube_screenshot.png" alt="youtube_screenshot.png" />
</p>
<p><span class="figure-number">Figure 1:</span> youtube<sub>screenshot</sub></p>
</div>

<p>
The video illustrates a car learning to avoid obstacles. 
As shown in Figure <a href="#orgparagraph1">1</a>, the environment is a 2D scenario.
The whole scenario is surrounded by fences.
There are 4 irregularly shaped obstacles.
To show the learned ability generalizes well with different layouts of obstacles,
the obstacles will revolve about the center of the room slowly,
at a constant speed, during running.
</p>

<p>
For each period, the car starts from the center of the room,
with a randomly chosen direction.
Its action is controlled by a reinforcement learning algorithm.
During the early periods, the actions are like randomly decided.
When the car runs into obstacles or fences,
it will be repositioned to the center of the room, and a new period begins.
The car learns over time.
</p>


<div id="orgparagraph2" class="figure">
<p><img src="images/youtube_structure.png" alt="youtube_structure.png" />
</p>
<p><span class="figure-number">Figure 2:</span> youtube<sub>structure</sub></p>
</div>

<p>
The illustration comprises two parts, a 2D environment described above,
and a learning algorithm which controls the action of the car.
The algorithm learns from and makes decisions on specified data provided by the environment.
The environment emulates 5 state sensors,
corresponding to 5 different direction in front of the car.
These sensors find the nearest obstacle,
and measure the distance from the obstacle to the car.
The distance information is transferred to the car in the form of a 5D vector.
During running, the algorithm will predict an action based on the distance information 
and then feed it back to the environment, to control the movement of the car. 
</p>
</div>
</div>

<div id="outline-container-orgheadline17" class="outline-3">
<h3 id="orgheadline17"><span class="section-number-3">1.3</span> Re-Implementation</h3>
<div class="outline-text-3" id="text-1-3">
<p>
For our further research, we need a verified code base to start with. 
So we want to implement the application in this video. 
</p>


<div id="orgparagraph3" class="figure">
<p><img src="images/reimplementation_structure.png" alt="reimplementation_structure.png" />
</p>
<p><span class="figure-number">Figure 3:</span> structure</p>
</div>

<p>
The implementation takes 3 steps.
</p>
<ul class="org-ul">
<li>Build an environment.</li>
<li>Implement a reinforcement learning algorithm to control the car.</li>
<li>Provide and transfer learning data and actions between the environment and the algorithm.</li>
</ul>
</div>

<div id="outline-container-orgheadline5" class="outline-4">
<h4 id="orgheadline5"><span class="section-number-4">1.3.1</span> Reinforcement Learning</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
We started this part with a project on Github.com.
In further research, we will build a variant algorithm of the deep q network.
So we choose this implementation of deep q network as the code base of our reinforcement learning algorithm.
This implementation is written in python, and based on Theano and Lasagne.
</p>

<dl class="org-dl">
<dt>Paper</dt><dd><a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a></dd>
<dt>Code</dt><dd><a href="https://github.com/spragunr/deep_q_rl">https://github.com/spragunr/deep_q_rl</a></dd>
</dl>
</div>
<div id="outline-container-orgheadline4" class="outline-5">
<h5 id="orgheadline4"><span class="section-number-5">1.3.1.1</span> Adaptation</h5>
<div class="outline-text-5" id="text-1-3-1-1">
<p>
The deep q network takes the raw images of a game as its inputs to predict actions.
The network includes convolutional layers to handle the image data.
But the current environment provides only distance information, 
in the form of a vector, 
instead of the raw images.
So we replaced the convolutional neural network with a simple multi-layer neural network,
to handle the distance information.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline10" class="outline-4">
<h4 id="orgheadline10"><span class="section-number-4">1.3.2</span> The Environment</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
We choose to build this 2D environment on a real-time 3D rendering engine. 
It is for the convenience of later transition.
Because our later goal is to train the car to avoid obstacles in 3D environments. 
</p>
</div>
<div id="outline-container-orgheadline7" class="outline-5">
<h5 id="orgheadline7"><span class="section-number-5">1.3.2.1</span> Panda3D</h5>
<div class="outline-text-5" id="text-1-3-2-1">
<p>
The 3D rendering task will be accomplished by Panda3D, a game development environment.
It is written in C++ and supports both C++ and python.
</p>
</div>
<div id="outline-container-orgheadline6" class="outline-6">
<h6 id="orgheadline6"><span class="section-number-6">1.3.2.1.1</span> Intra Process Communication</h6>
<div class="outline-text-6" id="text-1-3-2-1-1">
<p>
In future research, 
it is required to frequently transfer raw images from the environment to the learning model.
To make this process more efficient, we want to be able to transfer image data as an intra-process pointer.
Choosing python as the programming language for both the environment and the learning model 
will allows us to conveniently implement this mechanism.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline8" class="outline-5">
<h5 id="orgheadline8"><span class="section-number-5">1.3.2.2</span> Models</h5>
<div class="outline-text-5" id="text-1-3-2-2">
<p>
The 3D models used to build the 3D environment come from Panda3D's example games.
</p>
<ul class="org-ul">
<li>The models of the room are provided by the example bump-mapping.</li>
<li>The models of the chess pieces are provided by the example chessboard.</li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline9" class="outline-5">
<h5 id="orgheadline9"><span class="section-number-5">1.3.2.3</span> Layout</h5>
<div class="outline-text-5" id="text-1-3-2-3">

<div id="orgparagraph4" class="figure">
<p><img src="images/screenshot_first_person.png" alt="screenshot_first_person.png" />
</p>
<p><span class="figure-number">Figure 4:</span> A 3D first person view of the car</p>
</div>


<div id="orgparagraph5" class="figure">
<p><img src="images/screenshot_layout_2D.png" alt="screenshot_layout_2D.png" />
</p>
<p><span class="figure-number">Figure 5:</span> A top-down view. Red shapes represents obstacles. (mark the poles later)(Green circle/blue lines remove later) Green destination(remove)Blue line routes (remove)</p>
</div>

<p>
As shown in Figure <a href="#orgparagraph4">4</a>, the scenario is located in a cubic room.
A round pole is fixed at the left bottom corner (Figure <a href="#orgparagraph5">5</a>).
And a square pole is fixed at the top right corner.
200 chess pieces are randomly positioned in the room.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline12" class="outline-4">
<h4 id="orgheadline12"><span class="section-number-4">1.3.3</span> Learning Data</h4>
<div class="outline-text-4" id="text-1-3-3">

<div id="orgparagraph6" class="figure">
<p><img src="images/depth_map_1d.png" alt="depth_map_1d.png" />
</p>
<p><span class="figure-number">Figure 6:</span> cropping the horizontal line from the depth map</p>
</div>
</div>

<div id="outline-container-orgheadline11" class="outline-5">
<h5 id="orgheadline11"><span class="section-number-5">1.3.3.1</span> Depth Map</h5>
<div class="outline-text-5" id="text-1-3-3-1">

<div id="orgparagraph7" class="figure">
<p><img src="images/depth_map.png" alt="depth_map.png" />
</p>
<p><span class="figure-number">Figure 7:</span> A depth map</p>
</div>


<p>
·         Extracting depth maps (
Goal: E (distance) =&gt; Q )
Convenient Method: depth map =&gt; distance
首先,根据场景内的三维模型生成深度信息(<a href="https://en.wikipedia.org/wiki/Depth_map">https://en.wikipedia.org/wiki/Depth_map</a>) (Figure 2).
What is depth map? Figure ???
How to generate from 3d models
How to extract distance from depth map. Figure ???
</p>

<p>
然后,截取地平线的部分(Figure 3),作为对视频中的一维距离信息的模拟,发送给agent
在场景中,当agent遇到障碍物的时候,场景会发送惩罚信号给agent.
Figure 1: agent视角的虚拟三维环境,通过三维引擎的立体视觉模式加入了左右两个视角
Figure 2: depth map generated from 3d models
Figure 3: 截取地平线的部分生成一维的深度信息
o   Reimplement q-learning
o   Feeding distance to q-learning
Different Implementaion
o   Data: volume??
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline14" class="outline-4">
<h4 id="orgheadline14"><span class="section-number-4">1.3.4</span> Training</h4>
<div class="outline-text-4" id="text-1-3-4">
</div><div id="outline-container-orgheadline13" class="outline-5">
<h5 id="orgheadline13"><span class="section-number-5">1.3.4.1</span> Random Actions</h5>
</div>
</div>
<div id="outline-container-orgheadline16" class="outline-4">
<h4 id="orgheadline16"><span class="section-number-4">1.3.5</span> Results and discussion:</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
在满是障碍物的地图上,agent很明显的可以在较长的一段时间中避开障碍物. 但是并不能做到在所以情形下完全避开障碍物.
</p>
</div>
<div id="outline-container-orgheadline15" class="outline-5">
<h5 id="orgheadline15"><span class="section-number-5">1.3.5.1</span> Problems</h5>
<div class="outline-text-5" id="text-1-3-5-1">
<p>
我们缺乏有效的指标判断训练结果的有效性. 障碍物地图是随机生成的,有些情况下agent可能陷入无法避开障碍物的情形.但是我们缺乏手段评估哪些障碍物环境是属于这一类别的.
在参数调整的后期,很难根据agent不碰撞的时间长度来判断算法的优劣
</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2016-08-20 Sat 06:04</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
